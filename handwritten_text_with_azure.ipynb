{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siwarnasri/Snippet-Library/blob/main/handwritten_text_with_azure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tQvRmZLDSEO"
      },
      "source": [
        "# Recognizing and extracting handwritten text from images, using Microsoft Azure and Python\n",
        "\n",
        "This notebook contains the necessary code to send an image to Azure, get Azure to recognize and extract handwritten text, and send that text back to us.\n",
        "\n",
        "You are currently looking at a copy. To **run** this code, you need to be signed into a Google account, and then hit 'file' and 'save a copy in drive'. You'll be able to run your **copy**.\n",
        "\n",
        "## How this notebook works\n",
        "\n",
        "Each cell below contains an individual part of the complete program. We are going to run each cell in turn. A cell is **active** if it appears with a shadow around it (as if it was popped up a bit). If you mouse over an active cell where you see `[ ]`, a little 'run' button appears. You click that run button, and a little wait indicator appears (how long it appears depends on the complexity of the operation). **When the code is finished running** a number will appear between the brackets, like so: `[1]`. This tells you that this was cell number 1 to run.\n",
        "\n",
        "**Important:** Let a cell finish before running the next cell below it. If you run cells out of order, or try to run two cells at the same time, things can get screwed up.\n",
        "\n",
        "Got that? Ready? Then let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-Zqmlyk4mBy"
      },
      "source": [
        "## 1. Get the packages we need, import the bits we want.\n",
        "\n",
        "We only do this whenever we start this process. It only has to be done once per session.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S0_2KBq3txi"
      },
      "source": [
        "# First we go get the sdk:\n",
        "!pip install --upgrade azure-cognitiveservices-vision-computervision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiMhMG-Dz4mP"
      },
      "source": [
        "# https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/quickstarts-sdk/python-sdk\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import TextOperationStatusCodes\n",
        "from azure.cognitiveservices.vision.computervision.models import TextRecognitionMode\n",
        "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "\n",
        "from array import array\n",
        "import os\n",
        "from PIL import Image\n",
        "import sys\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J91Fgy2v4Xc3"
      },
      "source": [
        "## 2. Now we connect up with Azure\n",
        "\n",
        "We only do this during our start up, after we've done the bit above. **Notice lines 4 and 5 in the block below**, where it says `COMPUTER_VISION_SUBSCRIPTION_KEY` and `COMPUTER_VISION_ENDPOINT`. These are two variables that you need to complete with the information from the computer vision service you created in your Azure portal (a reminder of what this looks like is at [figure 7](shawngraham.github.io/dhmuse/detecting-handwriting/#a-shortcut).\n",
        "\n",
        "The key will be a long string of letters and numbers separated by hyphens; paste this **between** the `'` where it says `xxxx`. The endpoint will be a full url **with** the https bit. Paste that **between** the `'` where it says `yyyy`.\n",
        "\n",
        "Once you've done that, run the three blocks of code below in order.\n",
        "\n",
        "**PS** if you share this notebook, _delete_ the key and endpoint and replace with xxx and yyy before you share it. You don't want this info lying around."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxXEeUHqz33o"
      },
      "source": [
        "# you have to sign up for a free trial with azure, https://portal.azure.com\n",
        "# then make a resource under 'cognitive resources'\n",
        "# for computer vision to get the correct api, endpoint\n",
        "os.environ['COMPUTER_VISION_SUBSCRIPTION_KEY']='xxxx'\n",
        "os.environ['COMPUTER_VISION_ENDPOINT']='yyyy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2-XeDxx309x"
      },
      "source": [
        "# Add your Computer Vision subscription key to your environment variables.\n",
        "if 'COMPUTER_VISION_SUBSCRIPTION_KEY' in os.environ:\n",
        "    subscription_key = os.environ['COMPUTER_VISION_SUBSCRIPTION_KEY']\n",
        "else:\n",
        "    print(\"\\nSet the COMPUTER_VISION_SUBSCRIPTION_KEY environment variable.\\n**Restart your shell or IDE for changes to take effect.**\")\n",
        "    sys.exit()\n",
        "# Add your Computer Vision endpoint to your environment variables.\n",
        "if 'COMPUTER_VISION_ENDPOINT' in os.environ:\n",
        "    endpoint = os.environ['COMPUTER_VISION_ENDPOINT']\n",
        "else:\n",
        "    print(\"\\nSet the COMPUTER_VISION_ENDPOINT environment variable.\\n**Restart your shell or IDE for changes to take effect.**\")\n",
        "    sys.exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7RmGZkWBsUU"
      },
      "source": [
        "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxLtiUtVKzlk"
      },
      "source": [
        "## 3. Set up and load our target image.\n",
        "\n",
        "Now the fun begins. The two blocks below do two things.\n",
        "\n",
        "The first creates a variable called 'remote_image_url' and gives it the information of where our image lives.\n",
        "\n",
        "The second loads the image up and gets it ready for processing.\n",
        "\n",
        "Easiest solution: Put your images in a github repository. Once your image is uploaded to github, right-click it and select 'copy image url'. The resulting URL will follow this pattern: https://raw.githubusercontent.com/shawngraham/demo/master/frontenac-card.png  .  The 'raw.githubusercontent.com/your-name/your-repo/master/file-name.png' is what you want.\n",
        "\n",
        "Run the two code blocks in order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKQeqfJPCBBI"
      },
      "source": [
        "remote_image_url = \"http://data2.archives.ca/ap/c/c147108.jpg\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTEQR5cVKo9g",
        "outputId": "180b845e-ee5c-4173-8e26-941b8265f3f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''\n",
        "Batch Read File, recognize printed text - remote\n",
        "This example will extract printed text in an image, then print results, line by line.\n",
        "This API call can also recognize handwriting (not shown).\n",
        "'''\n",
        "print(\"===== Batch Read File - remote =====\")\n",
        "# Get an image with printed text\n",
        "remote_image_printed_text_url = remote_image_url\n",
        "# remote_image_printed_text_url = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/printed_text.jpg\"\n",
        "# Call API with URL and raw response (allows you to get the operation location)\n",
        "recognize_printed_results = computervision_client.batch_read_file(remote_image_printed_text_url,  raw=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== Batch Read File - remote =====\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzS7Te2s4z0o"
      },
      "source": [
        "## 4. Now OCR that handwriting!\n",
        "\n",
        "The next bit of code sends our image through the Azure computers. Just run it, and Azure will give us back the text that it has identified. Copy the text to a file somewhere, go back to step 3 and change up the remote_image_url, run those two blocks, then this one, to get your next image(s).\n",
        "\n",
        "ta da!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh__aBq4OabS",
        "outputId": "ae911b53-dd9b-4a40-b923-ab90a24fc8dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Get the operation location (URL with an ID at the end) from the response\n",
        "operation_location_remote = recognize_printed_results.headers[\"Operation-Location\"]\n",
        "# Grab the ID from the URL\n",
        "operation_id = operation_location_remote.split(\"/\")[-1]\n",
        "\n",
        "# Call the \"GET\" API and wait for it to retrieve the results\n",
        "while True:\n",
        "    get_printed_text_results = computervision_client.get_read_operation_result(operation_id)\n",
        "    if get_printed_text_results.status not in ['NotStarted', 'Running']:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# Print the detected text, line by line\n",
        "if get_printed_text_results.status == TextOperationStatusCodes.succeeded:\n",
        "    for text_result in get_printed_text_results.recognition_results:\n",
        "        for line in text_result.lines:\n",
        "            print(line.text)\n",
        "            #print(line.bounding_box)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Complete. So stripped la a bath . I found the\n",
            "briden had burst and there was no hot water In\n",
            "bathing . so of shaved smoothly - scrubbed my lice\n",
            "and neck and arms - brushed my teeth, and .. .\n",
            "bat down to speak to you and live with my\n",
            "Children , Bless you all - make yourallies as\n",
            "pure and sweet as flowers specked with crystal\n",
            "tar . Love with all your hearts . everything and\n",
            "beryone . What a game it is ! All of us , individually\n",
            "Going our way - a different outlooks in nearly all\n",
            "things and yet so Similar - Hour grand it is for\n",
            "them who have companions near to hem - I'm\n",
            "awful sorry for the lonely mes . They have it\n",
            "half the chance, but then they are given often more\n",
            "spirit to fight and their vision is very lived.\n",
            "hell . my sweet little wife . (I've cultwated a sense of\n",
            "Smell, and think of you as an odown . the same with\n",
            "my children ) the clocks have just struck midnight . I'm\n",
            "going to bed - to be guiltly with my own thoughts.\n",
            "to he content - Is make the most of time between work\n",
            "it's all way beautiful to ed so happy . This that has\n",
            "ame me good - I shall smile as I lay me down.\n",
            "perfectly content and calmly happy - Goodnight my darlings\n",
            "your towing one- fred.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnmB4Uik45gF"
      },
      "source": [
        "## 5. And of course, do other things too.\n",
        "\n",
        "You don't have to run these blocks, but you can try them out on other images if you want (again, change the remote_image_url in Step 3).\n",
        "\n",
        "You're welcome to try them out. All of this is for free, as long as you only do it at a rate of 20 per minute, and to a maximum of 5000 per month. Every time you see a comment in the code saying 'Call API', that's what we're talking about - we call the Azure computer using its API, in essence saying, 'hey, here's another image, what text is in it?'. That's a call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acwktMGjCUSe",
        "outputId": "bfd2ea54-bbb8-481f-8ba8-29af8f953b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "'''\n",
        "Describe an Image - remote\n",
        "This example describes the contents of an image with the confidence score.\n",
        "'''\n",
        "print(\"===== Describe an image - remote =====\")\n",
        "# Call API\n",
        "description_results = computervision_client.describe_image(remote_image_url )\n",
        "\n",
        "# Get the captions (descriptions) from the response, with confidence level\n",
        "print(\"Description of remote image: \")\n",
        "if (len(description_results.captions) == 0):\n",
        "    print(\"No description detected.\")\n",
        "else:\n",
        "    for caption in description_results.captions:\n",
        "        print(\"'{}' with confidence {:.2f}%\".format(caption.text, caption.confidence * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== Describe an image - remote =====\n",
            "Description of remote image: \n",
            "'a close up of text on a white background' with confidence 82.97%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlqjEsOhCmuE",
        "outputId": "bb7b7461-e6a5-4177-b984-88711c949e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "'''\n",
        "Categorize an Image - remote\n",
        "This example extracts (general) categories from a remote image with a confidence score.\n",
        "'''\n",
        "print(\"===== Categorize an image - remote =====\")\n",
        "# Select the visual feature(s) you want.\n",
        "remote_image_features = [\"categories\"]\n",
        "# Call API with URL and features\n",
        "categorize_results_remote = computervision_client.analyze_image(remote_image_url , remote_image_features)\n",
        "\n",
        "# Print results with confidence score\n",
        "print(\"Categories from remote image: \")\n",
        "if (len(categorize_results_remote.categories) == 0):\n",
        "    print(\"No categories detected.\")\n",
        "else:\n",
        "    for category in categorize_results_remote.categories:\n",
        "        print(\"'{}' with confidence {:.2f}%\".format(category.name, category.score * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== Categorize an image - remote =====\n",
            "Categories from remote image: \n",
            "'abstract_' with confidence 0.78%\n",
            "'others_' with confidence 0.39%\n",
            "'text_' with confidence 36.33%\n",
            "'text_menu' with confidence 22.27%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxB8R9b2CtMN",
        "outputId": "95361858-01f9-44ce-99c6-38f903a42401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "'''\n",
        "Tag an Image - remote\n",
        "This example returns a tag (key word) for each thing in the image.\n",
        "'''\n",
        "print(\"===== Tag an image - remote =====\")\n",
        "# Call API with remote image\n",
        "tags_result_remote = computervision_client.tag_image(remote_image_url )\n",
        "\n",
        "# Print results with confidence score\n",
        "print(\"Tags in the remote image: \")\n",
        "if (len(tags_result_remote.tags) == 0):\n",
        "    print(\"No tags detected.\")\n",
        "else:\n",
        "    for tag in tags_result_remote.tags:\n",
        "        print(\"'{}' with confidence {:.2f}%\".format(tag.name, tag.confidence * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== Tag an image - remote =====\n",
            "Tags in the remote image: \n",
            "'text' with confidence 99.93%\n",
            "'handwriting' with confidence 96.85%\n",
            "'letter' with confidence 92.40%\n",
            "'menu' with confidence 73.72%\n",
            "'document' with confidence 69.52%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9HfXY7bC6x4",
        "outputId": "d10b3dfb-85f1-4170-bf96-bb1a0dae7668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "'''\n",
        "Detect Objects - remote\n",
        "This example detects different kinds of objects with bounding boxes in a remote image.\n",
        "'''\n",
        "print(\"===== Detect Objects - remote =====\")\n",
        "# Get URL image with different objects\n",
        "remote_image_url_objects = \"http://data2.archives.ca/ap/c/c147108.jpg\"\n",
        "# Call API with URL\n",
        "detect_objects_results_remote = computervision_client.detect_objects(remote_image_url_objects)\n",
        "\n",
        "# Print detected objects results with bounding boxes\n",
        "print(\"Detecting objects in remote image:\")\n",
        "if len(detect_objects_results_remote.objects) == 0:\n",
        "    print(\"No objects detected.\")\n",
        "else:\n",
        "    for object in detect_objects_results_remote.objects:\n",
        "        print(\"object at location {}, {}, {}, {}\".format( \\\n",
        "        object.rectangle.x, object.rectangle.x + object.rectangle.w, \\\n",
        "        object.rectangle.y, object.rectangle.y + object.rectangle.h))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== Detect Objects - remote =====\n",
            "Detecting objects in remote image:\n",
            "No objects detected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKFM3D6DD5G-",
        "outputId": "a988c5e7-b68e-4427-92c1-16f00c7596bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Call API with content type (landmarks) and URL\n",
        "detect_domain_results_landmarks = computervision_client.analyze_image_by_domain(\"landmarks\", remote_image_url)\n",
        "print()\n",
        "\n",
        "print(\"Landmarks in the remote image:\")\n",
        "if len(detect_domain_results_landmarks.result[\"landmarks\"]) == 0:\n",
        "    print(\"No landmarks detected.\")\n",
        "else:\n",
        "    for landmark in detect_domain_results_landmarks.result[\"landmarks\"]:\n",
        "        print(landmark[\"name\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Landmarks in the remote image:\n",
            "No landmarks detected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2L30F3ED_-q",
        "outputId": "99fd4584-820a-4203-b887-5a6002951b49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "'''\n",
        "Detect Color - remote\n",
        "This example detects the different aspects of its color scheme in a remote image.\n",
        "'''\n",
        "print(\"===== Detect Color - remote =====\")\n",
        "# Select the feature(s) you want\n",
        "remote_image_features = [\"color\"]\n",
        "# Call API with URL and features\n",
        "detect_color_results_remote = computervision_client.analyze_image(remote_image_url, remote_image_features)\n",
        "\n",
        "# Print results of color scheme\n",
        "print(\"Getting color scheme of the remote image: \")\n",
        "print(\"Is black and white: {}\".format(detect_color_results_remote.color.is_bw_img))\n",
        "print(\"Accent color: {}\".format(detect_color_results_remote.color.accent_color))\n",
        "print(\"Dominant background color: {}\".format(detect_color_results_remote.color.dominant_color_background))\n",
        "print(\"Dominant foreground color: {}\".format(detect_color_results_remote.color.dominant_color_foreground))\n",
        "print(\"Dominant colors: {}\".format(detect_color_results_remote.color.dominant_colors))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== Detect Color - remote =====\n",
            "Getting color scheme of the remote image: \n",
            "Is black and white: False\n",
            "Accent color: 3B2417\n",
            "Dominant background color: Grey\n",
            "Dominant foreground color: Grey\n",
            "Dominant colors: ['Grey']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}